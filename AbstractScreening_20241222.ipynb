{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging LLMs for Abstract Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/LLMAbstractScreen/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.read_csv('./Dataset/Abstracts_CD008874_20241012_sample.csv')\n",
    "Examples = pd.read_csv('./Dataset/Abstracts_CD008874_20241012_examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetInput(row):\n",
    "    output = '\"\"\" ### Title '\n",
    "    output += row['title']\n",
    "    output += ' ### Abstract '\n",
    "    output += row['abstract']\n",
    "    output += '\"\"\"'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples['Input'] = Examples.apply(lambda row: GetInput(row), axis=1)  \n",
    "Dataset['Input'] = Dataset.apply(lambda row: GetInput(row), axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT tokenizer (uncased)\n",
    "transformer_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "\n",
    "# load pretrained BERT model\n",
    "model = BertModel.from_pretrained(transformer_name)\n",
    "\n",
    "# assign device (cuda if possible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBertCls(text):\n",
    "  '''\n",
    "  this function takes the following input:\n",
    "  text to be represented by the BERT CLS token\n",
    "  and gives the following output:\n",
    "  a numpy array representing the text\n",
    "  '''\n",
    "  tok_text = tokenizer(text[:512],\n",
    "                       return_tensors='pt').to(device)\n",
    "  mod_output = model(**tok_text,\n",
    "                     output_hidden_states=True)\n",
    "  last_hidden_states = mod_output.hidden_states[-1]\n",
    "  return last_hidden_states[:,0,:].cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with cls representation of document\n",
    "Examples['BERT_cls'] = Examples['Input'].apply(getBertCls)\n",
    "Dataset['BERT_cls'] = Dataset['Input'].apply(getBertCls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calCosSim(emb1, emb2):\n",
    "  '''\n",
    "  return the cosine similarity of the\n",
    "  2 input numpy array\n",
    "  '''\n",
    "  result = emb1 @ emb2.T\n",
    "  result /= (np.linalg.norm(emb1)*np.linalg.norm(emb2))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPosExample(row):\n",
    "    emb = row['BERT_cls']\n",
    "    PosExamples = Examples[Examples['Include_cont'] == 1].copy()\n",
    "    PMID = row['PMID']\n",
    "    PosExamples = PosExamples[PosExamples['PMID'] != PMID].copy()\n",
    "    PosExamples['Sim'] = PosExamples['BERT_cls'].apply(lambda x: calCosSim(emb, x))\n",
    "    sim = PosExamples['Sim']\n",
    "    \n",
    "    return PosExamples.iloc[np.argmax(sim)]['Input']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples['PosExample'] = Examples.apply(GetPosExample, axis=1)\n",
    "Dataset['PosExample'] = Dataset.apply(GetPosExample, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get key\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "### Instruction\n",
    "\n",
    "You will take the role of a scientific research reviewer to evaluate titles and abstracts for further assessment \\\n",
    "concerning the systematic review study titled \\\n",
    "\"Airway physical examination tests for detection of difficult airway management in apparently normal adult patients\" \\\n",
    "Please generate a relevance score for the title and abstract in triple quotes based on the following inclusion criteria. \\\n",
    "The relevance score should be between 0 (completely irrelevant) to 100 (perfectly relevant).\n",
    "\n",
    "### Inclusion Criteria\n",
    "\n",
    "1. Study Type: Full-text diagnostic test accuracy studies.\n",
    "\n",
    "2. Index Tests (one or more):\n",
    "   - Mallampati test\n",
    "   - Modified Mallampati test\n",
    "   - Wilson risk score\n",
    "   - Thyromental distance\n",
    "   - Sternomental distance\n",
    "   - Mouth opening test\n",
    "   - Upper lip bite test\n",
    "\n",
    "3. Target Condition: Difficult airway, defined by:\n",
    "   - Difficult face mask ventilation\n",
    "   - Difficult laryngoscopy\n",
    "   - Difficult tracheal intubation\n",
    "   - Failed intubation\n",
    "\n",
    "4. Participants:\n",
    "   - Adult patients\n",
    "   - No obvious airway abnormalities\n",
    "   - Standard laryngoscope used\n",
    "   - Standard tracheal tube used\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "Expected output format:\n",
    "\n",
    "{{\n",
    "    \"RelevanceScore\": int from 0 to 100,\n",
    "    \"Reason\": \"Brief explanation of the Relevance Score based on inclusion criteria\"\n",
    "}}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelScore(row):\n",
    "    txt = row['Input']\n",
    "    example = row['PosExample']\n",
    "    prompt_used = prompt + f'\\n\\n### Example \\n\\n Below is an example delimited by triple quotes that should definitely be included: \\n \"\"\"{example}\"\"\"'\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_used},\n",
    "        {\"role\": \"user\", \"content\": txt}\n",
    "        ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples['Results'] = Examples.apply(getRelScore, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parseScore(res):\n",
    "    try:\n",
    "        js = res[1:-1]\n",
    "        \n",
    "        results = json.loads(js)\n",
    "        \n",
    "        return results[\"RelevanceScore\"]\n",
    "    except:\n",
    "        try:\n",
    "            js = res[8:-4]\n",
    "        \n",
    "            results = json.loads(js)\n",
    "        \n",
    "            return results[\"RelevanceScore\"]\n",
    "        except:\n",
    "            return \"Error\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def parseReason(res):\n",
    "    try:\n",
    "        js = res[1:-1]\n",
    "        \n",
    "        results = json.loads(js)\n",
    "        \n",
    "        return results[\"Reason\"]\n",
    "    except:\n",
    "        try:\n",
    "            js = res[8:-4]\n",
    "        \n",
    "            results = json.loads(js)\n",
    "        \n",
    "            return results[\"Reason\"]\n",
    "        except:\n",
    "            return \"Error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples['RelevanceScore'] = Examples['Results'].apply(parseScore)    \n",
    "Examples['Reason'] = Examples['Results'].apply(parseReason)\n",
    "Examples.to_csv('./Dataset/DynamicOneShot_Training_20241222.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset['Results'] = Dataset.apply(getRelScore, axis=1)\n",
    "Dataset['RelevanceScore'] = Dataset['Results'].apply(parseScore)    \n",
    "Dataset['Reason'] = Dataset['Results'].apply(parseReason)\n",
    "Dataset.to_csv('./Dataset/DynamicOneShot_dev_20241222.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
